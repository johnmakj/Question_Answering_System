#configuration file for bert finetuning

bert_qa:
  n_epochs: 1
  batch_size: 2
  num_workers: 6
  learning_rate: 2e-5
  weight_decay: 1e-8
  model_name: bert-base-finetuned-squad
  token: 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

pretrained:
  model: BertForQuestionAnswering.from_pretrained("bert-base-uncased")
  tokenizer: BertTokenizerFast.from_pretrained("bert-base-uncased")